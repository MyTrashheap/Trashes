{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import autojit, prange\n",
    "import os\n",
    "from pandas import DataFrame, read_csv, read_table\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pandas import read_csv, DataFrame, read_table\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "from numba import autojit, prange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"../Corpora/TO_CREATE_VALID_UTTERANCE\"\n",
    "EMOTIONS_PATH = CSV_PATH + \"/emotions\"\n",
    "GOOGLE_NEWS_PATH = CSV_PATH + \"/GOOGLE_1_billion_word/heldout\"\n",
    "df_maps = {}\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotions\n",
    "for filename in os.listdir(EMOTIONS_PATH):\n",
    "    df_maps[i] = read_table(os.path.join(EMOTIONS_PATH,filename), header=None)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    df = df.dropna()\n",
    "    df = df.applymap(lambda x: x.rstrip())\n",
    "    df = df.applymap(lambda x: x.lstrip())\n",
    "    df = df.applymap(lambda x: x[0].title() + x[1: ])\n",
    "    df = df.applymap(lambda x: x + \".\" if x[-1] != \".\" else \"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utteranes(df, utterances):\n",
    "    for column in df.columns:\n",
    "        utterances += list(df[column].unique())\n",
    "    return utterances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@autojit\n",
    "def parralel(pre_data):\n",
    "    data = []\n",
    "    l = len(pre_data)\n",
    "    for i in prange(l):\n",
    "        data.append(preprocessing(pre_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = list(df_maps.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "parralel(pre_data)\n",
    "stop = time.time()\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "data=[]\n",
    "for d in pre_data:\n",
    "    data.append(preprocessing(d))\n",
    "stop = time.time()\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "data=[]\n",
    "data = Parallel(n_jobs=-1)(delayed(preprocessing)(d) for d in pre_data)\n",
    "stop = time.time()\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = []\n",
    "for df in data:\n",
    "    utterances += get_utteranes(df, utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "british_gloss = read_csv(\"british-english.csv\", header=None)\n",
    "usa_gloss = read_csv(\"american-english.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "british_gloss.columns = [\"a\"]\n",
    "usa_gloss.columns = [\"a\"]\n",
    "words = list(british_gloss.a.unique()) + list(\",./';[]<>?:{}!@#$%^&*()_+-=\") + list(usa_gloss.a.unique())\n",
    "words = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gg(utt, gloss):\n",
    "    oov = False\n",
    "#     try:\n",
    "    tknzr1 = word_tokenize(utt)\n",
    "    for token in tknzr1:\n",
    "        if token.lower() not in gloss:\n",
    "            oov = True\n",
    "            break\n",
    "    if not oov:\n",
    "        return utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_utt(data, gloss):\n",
    "    valid_utt = []\n",
    "    for utt in data:\n",
    "        oov = False\n",
    "        try:\n",
    "            tknzr1 = word_tokenize(utt)\n",
    "            for token in tknzr1:\n",
    "                if token.lower() not in gloss:\n",
    "                    oov = True\n",
    "                    break\n",
    "            if not oov:\n",
    "                valid_utt.append(utt)\n",
    "        except:\n",
    "            print(utt)\n",
    "    return valid_utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(utterances)//600000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "valid = get_valid_utt(utterances[:5000], words)\n",
    "stop = time.time()\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(utterances)// 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd = [utterances[i * 50: (i + 1) * 50] for i in range(2 * len(utterances)// 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "data=[]\n",
    "validd = Parallel(n_jobs=-1)(delayed(get_valid_utt)(utt, words) for utt in ddd)\n",
    "stop = time.time()\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = []\n",
    "for el in validd:\n",
    "    valid += el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
