{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer, ngrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pandas import read_csv\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_phrase(phrase):\n",
    "    tknzr = TweetTokenizer()\n",
    "    return tknzr.tokenize(phrase)\n",
    "\n",
    "\n",
    "def preprocess_corpus(corpus):\n",
    "    tab_corpus = []\n",
    "    for phrase in corpus\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellchecker_1(data, gold_text, noizy_text, tokenized=False):\n",
    "    if not tokenized:\n",
    "        g_tokenized_text = word_tokenize(gold_text)\n",
    "        n_tokenized_text = word_tokenize(noizy_text)\n",
    "    else:\n",
    "        g_tokenized_text = gold_text\n",
    "        n_tokenized_text = noizy_text\n",
    "        \n",
    "    print(len(g_tokenized_text), len(n_tokenized_text))\n",
    "        \n",
    "    tokenized_text = zip(g_tokenized_text, n_tokenized_text)\n",
    "    ngram_text = list(tokenized_text)#list(ngrams(tokenized_text, 1, pad_right=True, right_pad_symbol=\"p\"))\n",
    "    correct_data = []\n",
    "    glossary = list(set(g_tokenized_text))\n",
    "    \n",
    "    for phrase in data:\n",
    "        tokenized_phrase = ngrams(word_tokenize(phrase), 1)\n",
    "        correct_phrase = []\n",
    "        for token in tokenized_phrase:\n",
    "            token = token[0]\n",
    "            print(token)\n",
    "            \n",
    "            #teraz dla tych dwoch tokenow musze znalexc token maksymalizujacy prawdopodobienstwo p = p(t2| t1, c) * p(c| t1)\n",
    "            #p_1 = p(t2| t1, c), mialo byc t1c, zapisano t1t2\n",
    "            #p_2 = p(c| t1), zaobserwowano t1, jakie jest prawdopodobienstwo ze nastepnie bedzie c\n",
    "            #albo p=p(t1,t2|c) * p(c)\n",
    "            p_max = 0\n",
    "            c_max = \"\"\n",
    "            for c in glossary:\n",
    "                sub_grams = [gram for gram in ngram_text if gram[0] == c]\n",
    "                if sub_grams:\n",
    "#                     print(sub_grams)\n",
    "                    p_1 = len([gram for gram in sub_grams if gram[1] == token]) / len(sub_grams)\n",
    "                    p_2 = len(sub_grams)\n",
    "                    if p_1 * p_2 > p_max:\n",
    "                        p_max = p_1 * p_2\n",
    "                        c_max = c\n",
    "            correct_phrase.append(c_max)\n",
    "            \n",
    "        correct_data.append(\" \".join(correct_phrase))\n",
    "        \n",
    "    return correct_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrected_sentence(sentence):\n",
    "    tokenized_phrase = ngrams(word_tokenize(phrase), 2)\n",
    "        correct_phrase = []\n",
    "        for token1, token2 in tokenized_phrase:\n",
    "            print(token1, token2)\n",
    "            \n",
    "            #teraz dla tych dwoch tokenow musze znalexc token maksymalizujacy prawdopodobienstwo p = p(t2| t1, c) * p(c| t1)\n",
    "            #p_1 = p(t2| t1, c), mialo byc t1c, zapisano t1t2\n",
    "            #p_2 = p(c| t1), zaobserwowano t1, jakie jest prawdopodobienstwo ze nastepnie bedzie c\n",
    "            #albo p=p(t1,t2|c) * p(c)\n",
    "            p_max = 0\n",
    "            c_max = \"\"\n",
    "            for c in glossary:\n",
    "                sub_grams = [gram for gram in ngram_text if gram[1][0] == c]\n",
    "                if sub_grams:\n",
    "#                     print(sub_grams)\n",
    "                    p_1 = len([gram for gram in sub_grams if gram[0][1] == token1 and gram[1][1] == token2]) / len(sub_grams)\n",
    "                    p_2 = len(sub_grams)\n",
    "#                     print(p_1 * p_2)\n",
    "                    if p_1 * p_2 > p_max:\n",
    "                        p_max = p_1 * p_2\n",
    "                        c_max = c\n",
    "            correct_phrase.append(c_max)\n",
    "    return \" \".join(correct_phrase)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assumptions:\n",
    "#data is a list, data is given in phrases form - each element is a phrase\n",
    "#corpus is a dict, corpus is given in golden_phrases : set_of_miss_spelled_phrases form\n",
    "#p_corpus is a dict, is given in tokenized_golden_token: set_of_empirical_spelled_tokenized_phrases\n",
    "\n",
    "#X - word in the window, \n",
    "#Y - observed word in the pre-window\n",
    "#Z - observed word in the window\n",
    "#P = (X|YZ)\n",
    "# interpretation: observing the string, what expression most likely ends it\n",
    "\n",
    "\n",
    "def spellchecker_2(data, gold_text, noizy_text, tokenized=False):\n",
    "    if not tokenized:\n",
    "        g_tokenized_text = word_tokenize(gold_text)\n",
    "        n_tokenized_text = word_tokenize(noizy_text)\n",
    "    else:\n",
    "        g_tokenized_text = gold_text\n",
    "        n_tokenized_text = noizy_text\n",
    "        \n",
    "    print(len(g_tokenized_text), len(n_tokenized_text))\n",
    "        \n",
    "    tokenized_text = zip(g_tokenized_text, n_tokenized_text)\n",
    "    ngram_text = list(ngrams(tokenized_text, 2))\n",
    "    corrected_data = []\n",
    "    glossary = list(set(g_tokenized_text))\n",
    "    \n",
    "    corrected_data = Parallel(n_jobs=-1)(delayed(get_corrected_data)(sentence, glossary, ngram_text) for sentence in data)\n",
    "        \n",
    "    return corrected_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../Errors generator\"\n",
    "data = read_csv(os.path.join(PATH, \"valid_wrong_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = list(data[\"valid\"].values)\n",
    "misspelled = list(data[\"misspelled\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\". I love rtavelling with my pet.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = \" \".join(valid)\n",
    "m = \" \".join(misspelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spellchecker_2(data, v, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34186 34186\n",
      ".\n",
      "I\n",
      "love\n",
      "rtavelling\n",
      "with\n",
      "my\n",
      "pet\n",
      ".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['. I love travelling with my pet .']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellchecker_1(data, v, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'l', 'a'),\n",
       " ('l', 'a', 'h'),\n",
       " ('a', 'h', 'a'),\n",
       " ('h', 'a', 's'),\n",
       " ('a', 's', ' '),\n",
       " ('s', ' ', 'a'),\n",
       " (' ', 'a', 's'),\n",
       " ('a', 's', ' '),\n",
       " ('s', ' ', 'c'),\n",
       " (' ', 'c', 'a'),\n",
       " ('c', 'a', 't')]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(\"alahas as cat\", 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assumptions:\n",
    "#data is a list, data is given in phrases form - each element is a phrase\n",
    "#corpus is a dict, corpus is given in golden_phrases : set_of_miss_spelled_phrases form\n",
    "#p_corpus is a dict, is given in tokenized_golden_token: set_of_empirical_spelled_tokenized_phrases\n",
    "\n",
    "#X - word in the window, \n",
    "#Y - observed word in the pre-window\n",
    "#Z - observed word in the window\n",
    "#P = (X|YZ)\n",
    "# interpretation: observing the string, what expression most likely ends it\n",
    "\n",
    "\n",
    "def spellchecker_2(data, gold_text, noizy_text, tokenized=False):\n",
    "    if not tokenized:\n",
    "        g_tokenized_text = word_tokenize(gold_text)\n",
    "        n_tokenized_text = word_tokenize(noizy_text)\n",
    "    else:\n",
    "        g_tokenized_text = gold_text\n",
    "        n_tokenized_text = noizy_text\n",
    "        \n",
    "    print(len(g_tokenized_text), len(n_tokenized_text))\n",
    "        \n",
    "    tokenized_text = zip(g_tokenized_text, n_tokenized_text)\n",
    "    ngram_text = list(ngrams(tokenized_text, 2))\n",
    "    correct_data = []\n",
    "    pprint(list(ngram_text)[449])\n",
    "    glossary = list(set(g_tokenized_text))\n",
    "    \n",
    "    for phrase in data:\n",
    "        tokenized_phrase = ngrams(word_tokenize(phrase), 2)\n",
    "        correct_phrase = []\n",
    "        for token1, token2 in tokenized_phrase:\n",
    "            print(token1, token2)\n",
    "            \n",
    "            #teraz dla tych dwoch tokenow musze znalexc token maksymalizujacy prawdopodobienstwo p = p(t2| t1, c) * p(c| t1)\n",
    "            #p_1 = p(t2| t1, c), mialo byc t1c, zapisano t1t2\n",
    "            #p_2 = p(c| t1), zaobserwowano t1, jakie jest prawdopodobienstwo ze nastepnie bedzie c\n",
    "            #albo p=p(t1,t2|c) * p(c)\n",
    "            p_max = 0\n",
    "            c_max = \"\"\n",
    "            for c in glossary:\n",
    "                sub_grams = [gram for gram in ngram_text if gram[1][0] == c]\n",
    "                if sub_grams:\n",
    "#                     print(sub_grams)\n",
    "                    p_1 = len([gram for gram in sub_grams if gram[0][1] == token1 and gram[1][1] == token2]) / len(sub_grams)\n",
    "                    p_2 = len(sub_grams)\n",
    "#                     print(p_1 * p_2)\n",
    "                    if p_1 * p_2 > p_max:\n",
    "                        p_max = p_1 * p_2\n",
    "                        c_max = c\n",
    "            correct_phrase.append(c_max)\n",
    "            \n",
    "        correct_data.append(\" \".join(correct_phrase))\n",
    "        \n",
    "    return correct_data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
