{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from pandas import read_csv, DataFrame, read_table\n",
    "from random import shuffle\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "from numba import autojit, prange\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"../Corpora/TO_CREATE_VALID_UTTERANCE\"\n",
    "EMOTIONS_PATH = CSV_PATH + \"/emotions\"\n",
    "GOOGLE_NEWS_PATH = CSV_PATH + \"/GOOGLE_1_billion_word/heldout\"\n",
    "df_maps = {}\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotions\n",
    "anger = read_table(os.path.join(TXT_PATH, \"anger\"), header=None, names=[\"anger\"])\n",
    "fear = read_table(os.path.join(TXT_PATH, \"fear\"), header=None, names=[\"fear\"])\n",
    "joy = read_table(os.path.join(TXT_PATH, \"joy\"), header=None, names=[\"joy\"])\n",
    "love = read_table(os.path.join(TXT_PATH, \"love\"), header=None, names=[\"love\"])\n",
    "sadness = read_table(os.path.join(TXT_PATH, \"sadness\"), header=None, names=[\"sadness\"])\n",
    "suprise = read_table(os.path.join(TXT_PATH, \"surprise\"), header=None, names=[\"surprise\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotions\n",
    "for filename in os.listdir(EMOTIONS_PATH):\n",
    "    df_maps[i] = read_table(os.path.join(EMOTIONS_PATH, filename), header=None)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 news.en.heldout-00038-of-00050\n",
      "1 news.en.heldout-00035-of-00050\n",
      "2 news.en.heldout-00043-of-00050\n",
      "3 news.en.heldout-00042-of-00050\n",
      "4 news.en.heldout-00034-of-00050\n",
      "5 news.en.heldout-00028-of-00050\n",
      "6 news.en.heldout-00048-of-00050\n",
      "7 news.en.heldout-00023-of-00050\n",
      "8 news.en.heldout-00037-of-00050\n",
      "9 news.en.heldout-00017-of-00050\n",
      "10 news.en.heldout-00009-of-00050\n",
      "11 news.en.heldout-00015-of-00050\n",
      "12 news.en.heldout-00040-of-00050\n",
      "13 news.en.heldout-00033-of-00050\n",
      "14 news.en.heldout-00030-of-00050\n",
      "15 news.en.heldout-00016-of-00050\n",
      "16 news.en.heldout-00026-of-00050\n",
      "17 news.en.heldout-00021-of-00050\n",
      "18 news.en.heldout-00006-of-00050\n",
      "19 news.en.heldout-00018-of-00050\n",
      "20 news.en.heldout-00008-of-00050\n",
      "21 news.en.heldout-00039-of-00050\n",
      "22 news.en.heldout-00046-of-00050\n",
      "23 news.en.heldout-00010-of-00050\n",
      "24 news.en.heldout-00036-of-00050\n",
      "25 news.en.heldout-00031-of-00050\n",
      "26 news.en.heldout-00003-of-00050\n",
      "27 news.en.heldout-00047-of-00050\n",
      "28 news.en.heldout-00014-of-00050\n",
      "29 news.en.heldout-00049-of-00050\n",
      "30 news.en.heldout-00001-of-00050\n",
      "31 news.en.heldout-00020-of-00050\n",
      "32 news.en.heldout-00004-of-00050\n",
      "33 news.en.heldout-00000-of-00050\n",
      "34 news.en.heldout-00013-of-00050\n",
      "35 news.en.heldout-00002-of-00050\n",
      "36 news.en.heldout-00029-of-00050\n",
      "37 news.en.heldout-00007-of-00050\n",
      "38 news.en-00000-of-00100\n",
      "39 news.en.heldout-00011-of-00050\n",
      "40 news.en.heldout-00032-of-00050\n",
      "41 news.en.heldout-00024-of-00050\n",
      "42 news.en.heldout-00005-of-00050\n",
      "43 news.en.heldout-00027-of-00050\n"
     ]
    }
   ],
   "source": [
    "#Google News\n",
    "for filename in os.listdir(GOOGLE_NEWS_PATH):\n",
    "    print(i, filename)\n",
    "    df_maps[i] = read_table(os.path.join(GOOGLE_NEWS_PATH, filename), header=None, names=\"{}\".format(i))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_flights = read_csv(os.path.join(CSV_PATH, \"Utterance-Flights-f1197494.csv\"))\n",
    "sentence_pairs = read_csv(os.path.join(CSV_PATH, \"1377882923_sentence_pairs.csv\"))\n",
    "headlines = read_csv(os.path.join(CSV_PATH, \"examiner-date-text.csv\"))\n",
    "# movie_lines = read_csv(os.path.join(CSV_PATH, \"movie_lines.csv\"), delimiter=\"+\")\n",
    "# movie_quoats = read_csv(os.path.join(CSV_PATH, \"moviequotes.scripts.csv\"), delimiter=\"+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_utt = DataFrame(utterance_flights, columns=[\"response_1\", \"response_2\", \"response_3\", \"scenario\"])\n",
    "chosen_sent = DataFrame(sentence_pairs, columns=[\"sentenceA\", \"sentenceB\"])\n",
    "chosen_headlines = DataFrame(headlines, columns=[\"headline_text\"])\n",
    "# chosen_lines = DataFrame(movie_lines, columns=[\"They do not!\"])\n",
    "# chosen_quoats = DataFrame(movie_quoats, columns=[\"Ladies and gentlemen\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = [\n",
    "    anger,\n",
    "    fear,\n",
    "    joy,\n",
    "    love,\n",
    "    sadness,\n",
    "    suprise,\n",
    "    chosen_utt,\n",
    "    chosen_sent,\n",
    "    chosen_headlines\n",
    "#     chosen_lines,\n",
    "#     chosen_quoats\n",
    "]\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, column):\n",
    "    df = df.dropna()\n",
    "    df = df.applymap(lambda x: x.rstrip())\n",
    "    df = df[df[column].str.len() > 6]\n",
    "    df = df.applymap(lambda x: x.lstrip())\n",
    "    df = df.applymap(lambda x: x[0].title() + x[1: ])\n",
    "    df = df.applymap(lambda x: x + \".\" if x[-1] != \".\" else \"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = []\n",
    "\n",
    "def get_utteranes(df, utterances):\n",
    "    for column in df.columns:\n",
    "        utterances += list(df[column].unique())\n",
    "    return utterances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for d in pre_data:\n",
    "    pre = list(pre_data[d])\n",
    "    data.append(preprocessing(pre, str(d)))\n",
    "\n",
    "for df in data:\n",
    "    utterances += get_utteranes(df, utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.pickle\", \"bw\") as f:\n",
    "    pickle.dump(utterances, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.pickle\", \"br\") as f:\n",
    "    utterances = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "british_gloss = read_csv(\"british-english.csv\", header=None)\n",
    "usa_gloss = read_csv(\"american-english.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "british_gloss.columns = [\"a\"]\n",
    "usa_gloss.columns = [\"a\"]\n",
    "words = list(british_gloss.a.unique()) + list(\",./';[]<>?:{}!@#$%^&*()_+-=\") + list(usa_gloss.a.unique())\n",
    "words = list(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utterances Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_utt(data, gloss):\n",
    "    valid_utt = []\n",
    "    un_valid = []\n",
    "    for utt in data:\n",
    "        oov = False\n",
    "        try:\n",
    "            tknzr1 = word_tokenize(utt)\n",
    "            for token in tknzr1:\n",
    "                if token.lower() not in gloss:\n",
    "                    oov = True\n",
    "                    break\n",
    "            if not oov:\n",
    "                valid_utt.append(utt)\n",
    "            else:\n",
    "                un_valid.append(utt)\n",
    "        except:\n",
    "            print(utt)\n",
    "#     return valid_utt, un_valid\n",
    "    return valid_utt\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd = [utterances[i * 50: (i + 1) * 50] for i in range(2 * len(utterances))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "validd = Parallel(n_jobs=-1)(delayed(get_valid_utt)(utt, words) for utt in ddd)\n",
    "stop = time.time()\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = []\n",
    "for el in validd:\n",
    "    valid += el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame(valid).to_csv(\"valid_utterances.csv\", index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
