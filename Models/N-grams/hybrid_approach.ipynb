{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hybrid model\n",
    "from nltk.tokenize import word_tokenize, ToktokTokenizer\n",
    "from nltk import ngrams\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from pandas import DataFrame, read_csv, read_table\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level N-Grams predicting + words distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../Corpora/TO_CREATE_VALID_UTTERANCE/GOOGLE_1_billion_word/heldout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(PATH, n):\n",
    "    bi_grams = defaultdict(int)\n",
    "    for filename in os.listdir(PATH):\n",
    "        with open(os.path.join(PATH, filename), \"r\") as f:\n",
    "            for line in f:\n",
    "                bi = ngrams(word_tokenize(line), n)\n",
    "                for token in bi:\n",
    "                    bi_grams[token] += 1\n",
    "       \n",
    "    return bi_grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramed = get_ngrams(PATH, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(a, key=lambda x: a[x], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4749409"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top(tab, p, c):\n",
    "    if len(tab) == 0:\n",
    "        return tab.append((c,p))\n",
    "    low = len(tab)-1\n",
    "    top = 0\n",
    "    while top < low:\n",
    "        pom = (low-top) // 2\n",
    "        if tab[pom][1] > p:\n",
    "            top = pom\n",
    "        else:\n",
    "            low = pom\n",
    "    return tab[: low] [(c,p)] + tab[low: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wykorzystaj rożne metryki, edit_distance, cosinusowe_z_embeddingów, jaccard, Jaro-Winkler distance\n",
    "def get_closest(tokens, token):\n",
    "    best_token = tokens[0]\n",
    "    min_dist = edit_distance(best_token, token)\n",
    "    for t in tokens[1: ]:\n",
    "        if edit_distance(t, token) < min_dist:\n",
    "            min_dist = edit_distance(t, token)\n",
    "            best_token = t\n",
    "    print(best_token, min_dist)\n",
    "    return best_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sprawdz jeszcze z opcją by w topce umieszczać tokeny zaczynające się lub kończące na tą samą literę\n",
    "# zwiększ topke, może top100\n",
    "#popraw ostatni krok z joinem\n",
    "def find_and_check(sentence, glossary, grams, n):\n",
    "    approx_sentence = []\n",
    "    gramed_sentence = list(ngrams(word_tokenize(sentence), n))\n",
    "    for token1, token2, token3 in gramed_sentence:\n",
    "#         sub_grams = [gram for gram in grams if gram[0] == token1 and gram[1] == token2]\n",
    "        sub_grams = [gram for gram in grams if g]\n",
    "        top10 = ['']\n",
    "        for c in glossary:\n",
    "            p = sum([sub_grams[gram] for gram in sub_grams if gram[3] == c]) / sum([sub_grams[gram] for gram in sub_grams])\n",
    "            if top10[0][n-1] < p:\n",
    "                top10 = get_top(top10, p, c)\n",
    "        tokens, dist = list(zip(*top10))\n",
    "        approx = get_closest(tokens, token3)\n",
    "        approx_sentence.append(approx)\n",
    "    return \" \".join(approx_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../N-grams\"\n",
    "df = read_csv(os.path.join(PATH, \"valid_utterances1.csv\"))\n",
    "# valid = list(df[\"valid\"].values)\n",
    "# df = DataFrame(valid)\n",
    "df.columns = [\"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:100000]\n",
    "data = [\"I am suppoed too just fel like he is sincere\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams, glossary = create_ngrams_from_text(df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_data = Parallel(n_jobs=-1)(delayed(find_and_check)(sentence, words, gramed, 3) for sentence in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method WordListCorpusReader.words of <WordListCorpusReader in '/home/user/nltk_data/corpora/words'>>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../N-grams/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "british_gloss = read_csv(PATH + \"british-english.csv\", header=None)\n",
    "british_gloss.columns = [\"word\"]\n",
    "usa_gloss = read_csv(PATH + \"american-english.csv\", header=None)\n",
    "usa_gloss.columns = [\"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(british_gloss.word.unique()) + list(\",./';[]<>?:{}!@#$%^&*()_+-=\") + list(usa_gloss.word.unique())\n",
    "words = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
