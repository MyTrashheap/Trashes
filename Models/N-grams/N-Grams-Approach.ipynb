{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer, ngrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, ToktokTokenizer\n",
    "from pandas import read_csv\n",
    "import os\n",
    "import time\n",
    "from pprint import pprint\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_phrase(phrase):\n",
    "    tknzr = TweetTokenizer()\n",
    "    return tknzr.tokenize(phrase)\n",
    "\n",
    "\n",
    "def preprocess_corpus(corpus):\n",
    "    tab_corpus = []\n",
    "    for phrase in corpus\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellchecker_1(data, gold_text, noizy_text, tokenized=False):\n",
    "    if not tokenized:\n",
    "        g_tokenized_text = word_tokenize(gold_text)\n",
    "        n_tokenized_text = word_tokenize(noizy_text)\n",
    "    else:\n",
    "        g_tokenized_text = gold_text\n",
    "        n_tokenized_text = noizy_text\n",
    "        \n",
    "    print(len(g_tokenized_text), len(n_tokenized_text))\n",
    "        \n",
    "    tokenized_text = zip(g_tokenized_text, n_tokenized_text)\n",
    "    ngram_text = list(tokenized_text)#list(ngrams(tokenized_text, 1, pad_right=True, right_pad_symbol=\"p\"))\n",
    "    correct_data = []\n",
    "    glossary = list(set(g_tokenized_text))\n",
    "    \n",
    "    for phrase in data:\n",
    "        tokenized_phrase = ngrams(word_tokenize(phrase), 1)\n",
    "        correct_phrase = []\n",
    "        for token in tokenized_phrase:\n",
    "            token = token[0]\n",
    "            print(token)\n",
    "            \n",
    "            #teraz dla tych dwoch tokenow musze znalexc token maksymalizujacy prawdopodobienstwo p = p(t2| t1, c) * p(c| t1)\n",
    "            #p_1 = p(t2| t1, c), mialo byc t1c, zapisano t1t2\n",
    "            #p_2 = p(c| t1), zaobserwowano t1, jakie jest prawdopodobienstwo ze nastepnie bedzie c\n",
    "            #albo p=p(t1,t2|c) * p(c)\n",
    "            p_max = 0\n",
    "            c_max = \"\"\n",
    "            for c in glossary:\n",
    "                sub_grams = [gram for gram in ngram_text if gram[0] == c]\n",
    "                if sub_grams:\n",
    "#                     print(sub_grams)\n",
    "                    p_1 = len([gram for gram in sub_grams if gram[1] == token]) / len(sub_grams)\n",
    "                    p_2 = len(sub_grams)\n",
    "                    if p_1 * p_2 > p_max:\n",
    "                        p_max = p_1 * p_2\n",
    "                        c_max = c\n",
    "            correct_phrase.append(c_max)\n",
    "            \n",
    "        correct_data.append(\" \".join(correct_phrase))\n",
    "        \n",
    "    return correct_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_table(c, ngram_text, token1, token2):\n",
    "    sub_grams = [gram for gram in ngram_text if gram[1][0] == c]\n",
    "    if sub_grams:\n",
    "        p_1 = len([gram for gram in sub_grams if gram[0][1] == token1 and gram[1][1] == token2]) / len(sub_grams)\n",
    "        p_2 = len(sub_grams)\n",
    "#                     print(p_1 * p_2)\n",
    "#                     print(sub_grams)\n",
    "        if p_1 * p_2 > 0:\n",
    "            return p_1 * p_2, c\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #teraz dla tych dwoch tokenow musze znalexc token maksymalizujacy prawdopodobienstwo p = p(t2| t1, c) * p(c| t1)\n",
    "#p_1 = p(t2| t1, c), mialo byc t1c, zapisano t1t2\n",
    "#p_2 = p(c| t1), zaobserwowano t1, jakie jest prawdopodobienstwo ze nastepnie bedzie c\n",
    "#albo p=p(t1,t2|c) * p(c)\n",
    "\n",
    "def get_corrected_sentence(sentence, ngram_text, glossary, tokenizer):\n",
    "    tokenized_phrase = list(ngrams(tokenizer.tokenize(sentence), 2))\n",
    "    print(list(tokenized_phrase))\n",
    "    correct_phrase = []\n",
    "    dist_tab = set()\n",
    "    for token1, token2 in tokenized_phrase:\n",
    "        dist_tab = Parallel(n_jobs=-1)(delayed(get_prob_table)(c, ngram_text, token1, token2) for c in glossary)\n",
    "        dist_tab = list(filter(lambda x: x is not None, dist_tab))\n",
    "        print(dist_tab, len(dist_tab))\n",
    "        if dist_tab:\n",
    "            a = sorted(dist_tab, reverse=True)\n",
    "            correct_phrase.append(a[0][1])\n",
    "    return \" \".join(correct_phrase)\n",
    "#detokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assumptions:\n",
    "#data is a list, data is given in phrases form - each element is a phrase\n",
    "#corpus is a dict, corpus is given in golden_phrases : set_of_miss_spelled_phrases form\n",
    "#p_corpus is a dict, is given in tokenized_golden_token: set_of_empirical_spelled_tokenized_phrases\n",
    "\n",
    "#X - word in the window, \n",
    "#Y - observed word in the pre-window\n",
    "#Z - observed word in the window\n",
    "#P = (X|YZ)\n",
    "# interpretation: observing the string, what expression most likely ends it\n",
    "\n",
    "\n",
    "def spellchecker(data, ngram_text, n):   \n",
    "    corrected_data = []\n",
    "    toktok = ToktokTokenizer()\n",
    "    \n",
    "    for sentence in data:\n",
    "        corrected_data.append(get_corrected_sentence(sentence, ngram_text, glossary, toktok))        \n",
    "    \n",
    "    return corrected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams_from_text(df_duo, n):\n",
    "    toktok = ToktokTokenizer()\n",
    "    valid = list(df_duo[\"valid\"].values)\n",
    "    misspelled = list(df_duo[\"misspelled\"].values)\n",
    "    gold_text = \" \".join(valid)\n",
    "    noizy_text = \" \".join(misspelled)\n",
    "    \n",
    "    g_tokenized_text = toktok.tokenize(gold_text)\n",
    "    n_tokenized_text = toktok.tokenize(noizy_text)\n",
    "        \n",
    "    print(len(g_tokenized_text), len(n_tokenized_text))\n",
    "    glossary = list(set(g_tokenized_text))\n",
    "        \n",
    "    tokenized_text = zip(g_tokenized_text, n_tokenized_text)\n",
    "    ngram_text = list(ngrams(tokenized_text, n))\n",
    "    \n",
    "    return ngram_text, glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../Errors generator\"\n",
    "df_duo = read_csv(os.path.join(PATH, \"valid_wrong_data1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_text, glossary = create_ngrams_from_text(df_duo, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\". I hate that feeling where you ffeel so \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "spellchecker(data, ngram_text[:10000], glossary)\n",
    "stop = time.time()\n",
    "print((stop - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spellchecker_1(data, v, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ngrams(\"alahas as cat\", 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assumptions:\n",
    "#data is a list, data is given in phrases form - each element is a phrase\n",
    "#corpus is a dict, corpus is given in golden_phrases : set_of_miss_spelled_phrases form\n",
    "#p_corpus is a dict, is given in tokenized_golden_token: set_of_empirical_spelled_tokenized_phrases\n",
    "\n",
    "#X - word in the window, \n",
    "#Y - observed word in the pre-window\n",
    "#Z - observed word in the window\n",
    "#P = (X|YZ)\n",
    "# interpretation: observing the string, what expression most likely ends it\n",
    "\n",
    "\n",
    "def spellchecker_2(data, gold_text, noizy_text, tokenized=False):\n",
    "    if not tokenized:\n",
    "        g_tokenized_text = word_tokenize(gold_text)\n",
    "        n_tokenized_text = word_tokenize(noizy_text)\n",
    "    else:\n",
    "        g_tokenized_text = gold_text\n",
    "        n_tokenized_text = noizy_text\n",
    "        \n",
    "    print(len(g_tokenized_text), len(n_tokenized_text))\n",
    "        \n",
    "    tokenized_text = zip(g_tokenized_text, n_tokenized_text)\n",
    "    ngram_text = list(ngrams(tokenized_text, 2))\n",
    "    correct_data = []\n",
    "    pprint(list(ngram_text)[449])\n",
    "    glossary = list(set(g_tokenized_text))\n",
    "    \n",
    "    for phrase in data:\n",
    "        tokenized_phrase = ngrams(word_tokenize(phrase), 2)\n",
    "        correct_phrase = []\n",
    "        for token1, token2 in tokenized_phrase:\n",
    "            print(token1, token2)\n",
    "            \n",
    "            #teraz dla tych dwoch tokenow musze znalexc token maksymalizujacy prawdopodobienstwo p = p(t2| t1, c) * p(c| t1)\n",
    "            #p_1 = p(t2| t1, c), mialo byc t1c, zapisano t1t2\n",
    "            #p_2 = p(c| t1), zaobserwowano t1, jakie jest prawdopodobienstwo ze nastepnie bedzie c\n",
    "            #albo p=p(t1,t2|c) * p(c)\n",
    "            p_max = 0\n",
    "            c_max = \"\"\n",
    "            for c in glossary:\n",
    "                sub_grams = [gram for gram in ngram_text if gram[1][0] == c]\n",
    "                if sub_grams:\n",
    "#                     print(sub_grams)\n",
    "                    p_1 = len([gram for gram in sub_grams if gram[0][1] == token1 and gram[1][1] == token2]) / len(sub_grams)\n",
    "                    p_2 = len(sub_grams)\n",
    "#                     print(p_1 * p_2)\n",
    "                    if p_1 * p_2 > p_max:\n",
    "                        p_max = p_1 * p_2\n",
    "                        c_max = c\n",
    "            correct_phrase.append(c_max)\n",
    "            \n",
    "        correct_data.append(\" \".join(correct_phrase))\n",
    "        \n",
    "    return correct_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sorted([0,2,4,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
