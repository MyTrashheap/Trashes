{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import itertools\n",
    "from pandas import read_csv, DataFrame, read_table\n",
    "from random import shuffle\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "from numba import autojit, prange\n",
    "from joblib import Parallel, delayed\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lenght_of_generator(generator):\n",
    "    counter = 0\n",
    "    new_generator = iter([])\n",
    "    \n",
    "    for el in generator:\n",
    "        counter += 1\n",
    "        new_generator = itertools.chain(new_generator, [el])\n",
    "    print(counter)\n",
    "    \n",
    "    return new_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_values_in_generator(generator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"../Corpora/TO_CREATE_VALID_UTTERANCE\"\n",
    "EMOTIONS_PATH = CSV_PATH + \"/emotions\"\n",
    "GOOGLE_NEWS_PATH = CSV_PATH + \"/GOOGLE_1_billion_word/heldout\"\n",
    "df_gen = iter([])\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotions\n",
    "for filename in os.listdir(EMOTIONS_PATH):\n",
    "    df = read_table(os.path.join(EMOTIONS_PATH, filename), header=None)\n",
    "    df.columns = [str(i)]\n",
    "    df_gen = itertools.chain(df_gen, [df])\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google News\n",
    "for filename in os.listdir(GOOGLE_NEWS_PATH):\n",
    "    print(i, filename)\n",
    "    df = read_table(os.path.join(GOOGLE_NEWS_PATH, filename), header=None)\n",
    "    df.columns = [str(i)]\n",
    "    df_gen = itertools.chain(df_gen, [df])\n",
    "    i += 1\n",
    "#     if i > 40:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_flights = read_csv(os.path.join(CSV_PATH, \"Utterance-Flights-f1197494.csv\"))\n",
    "sentence_pairs = read_csv(os.path.join(CSV_PATH, \"1377882923_sentence_pairs.csv\"))\n",
    "headlines = read_csv(os.path.join(CSV_PATH, \"examiner-date-text.csv\"))\n",
    "movie_lines = read_csv(os.path.join(CSV_PATH, \"movie_lines.csv\"), delimiter=\"+\")\n",
    "movie_quoats = read_csv(os.path.join(CSV_PATH, \"moviequotes.scripts.csv\"), delimiter=\"+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_utt = DataFrame(utterance_flights, columns=[\"response_1\", \"response_2\", \"response_3\", \"scenario\"])\n",
    "chosen_sent = DataFrame(sentence_pairs, columns=[\"sentenceA\", \"sentenceB\"])\n",
    "chosen_headlines = DataFrame(headlines, columns=[\"headline_text\"])\n",
    "chosen_lines = DataFrame(movie_lines, columns=[\"They do not!\"])\n",
    "chosen_quoats = DataFrame(movie_quoats, columns=[\"Ladies and gentlemen\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, column):\n",
    "    df = df.dropna()\n",
    "    df = df[df[column].str.len() > 10]\n",
    "    df = df.applymap(lambda x: x.lstrip())\n",
    "    df = df.applymap(lambda x: x[0].title() + x[1: ])\n",
    "    df = df.applymap(lambda x: x + \".\" if x[-1] not in \".?!\" else x)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utteranes(df, utterances):\n",
    "    for column in df.columns:\n",
    "        utterances = itertools.chain(utterances, list(df[column].unique()))\n",
    "    return utterances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = iter([])\n",
    "data = Parallel(n_jobs=-1)(delayed(preprocessing)(df, str(i)) for i, df in enumerate(df_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in data:\n",
    "    utterances = itertools.chain(utterances, get_utteranes(df, utterances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = count_lenght_of_generator(utterances)\n",
    "# unique_utterance = list(set(utterances))\n",
    "# len(unique_utterance)\n",
    "# shuffle(utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances, utterances1 = itertools.tee(utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.pickle\", \"bw\") as f:\n",
    "    pickle.dump(utterances, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.pickle\", \"br\") as f:\n",
    "    utterances = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### problem z nazwami własnymi, one mogą być dowolne, ale mogę użyć NERa i zamienić je na xxxxx, następnie nimi się nie przejować z korekcie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "british_gloss = read_csv(\"british-english.csv\", header=None)\n",
    "british_gloss.columns = [\"word\"]\n",
    "usa_gloss = read_csv(\"american-english.csv\", header=None)\n",
    "usa_gloss.columns = [\"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(british_gloss.word.unique()) + list(\",./';[]<>?:{}!@#$%^&*()_+-=\") + list(usa_gloss.word.unique()) + vocab\n",
    "words = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# napisać program ktory wyrzuca slowa ktore nie powtarzają się w generatorze, o wiele wydajniejsze niż counter\n",
    "def get_vocabulary(utterances):\n",
    "    toktok = ToktokTokenizer()\n",
    "    vocab = iter([])\n",
    "    \n",
    "    start = time.time()\n",
    "    voc = Parallel(n_jobs=-1)(delayed(toktok.tokenize)(utt.lower()) for utt in utterances)\n",
    "    stop = time.time()\n",
    "    print((stop - start))\n",
    "    \n",
    "    start = time.time()\n",
    "    for el in voc:\n",
    "        vocab = itertools.chain(vocab, el)\n",
    "    stop = time.time()\n",
    "    print((stop - start))\n",
    "    \n",
    "    return vocab\n",
    "    \n",
    "#     counter = Counter(vocab)\n",
    "#     vocab = (token for token in counter if counter[token] > 1)\n",
    "#     rare_vocab = (token for token in counter if counter[token] == 1)\n",
    "    \n",
    "#     return vocab, rare_vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "vocab = get_vocabulary(utterances)\n",
    "chunks = get_chunks(vocab, 10000)\n",
    "q = reduce(lambda x,y: iter(set(itertools.chain(x,y))), tqdm(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "vocab, rare_words = get_vocabulary(utterances)\n",
    "stop = time.time()\n",
    "print((stop - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time vocab = count_lenght_of_generator(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utterances Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_utt(chunk, vocab):\n",
    "    valid_utt = iter([])\n",
    "    un_valid = iter([])\n",
    "    toktok = ToktokTokenizer()\n",
    "    for utt in chunk:\n",
    "        oov = False\n",
    "        try:\n",
    "#             tknzr = word_tokenize(utt)\n",
    "            tknzr = toktok.tokenize(utt)\n",
    "            for token in tknzr:\n",
    "                if token.lower() not in vocab:\n",
    "                    oov = True\n",
    "                    break\n",
    "            if not oov:\n",
    "                valid_utt = itertools.chain(valid_utt, [utt])\n",
    "            else:\n",
    "                un_valid = itertools.chain(un_valid, [utt])\n",
    "        except:\n",
    "            print(utt)\n",
    "    return (valid_utt, un_valid)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(generator, chunk_size):\n",
    "    counter = 1\n",
    "    chunk = iter([])\n",
    "    for el in generator:\n",
    "        if counter > chunk_size:\n",
    "            yield chunk\n",
    "            chunk = iter([])\n",
    "            counter = 1\n",
    "\n",
    "        chunk = itertools.chain(chunk, [el])\n",
    "        counter += 1\n",
    "        \n",
    "    yield chunk\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = get_chunks(utterances, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "pair = Parallel(n_jobs=-1)(delayed(get_valid_utt)(chunk, words) for chunk in chunks)\n",
    "stop = time.time()\n",
    "print((stop - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair, pair1 = itertools.tee(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validd, _ = zip(*pair1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = iter([])\n",
    "for el in validd:\n",
    "    valid = itertools.chain(valid, el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = count_lenght_of_generator(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame(list(utterances)).to_csv(\"valid_utterances_all.csv\", index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ogarnij słowa z google news, wyrzuć te które pojawiają się tylko raz\n",
    "# jak efektywnie wgać generatory do DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(utterances1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
